{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mC-RjkqHIZJgxBsTIrHRpzvAJbL5g69R","timestamp":1752231676377}],"machine_shape":"hm","gpuType":"V28","toc_visible":true,"authorship_tag":"ABX9TyNVzBNXEstEX7zOmT4yaUYF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# **01 - GRID-BASED VS30 ESTIMATION**"],"metadata":{"id":"6JOZwxCRsXWg"}},{"cell_type":"markdown","source":["**IRDR0012 MSc Independent Research Project**\n","\n","*   Candidate number: NWHL6\n","*   Institution: UCL IRDR\n","*   Supervisor: Dr. Roberto Gentile\n","*   Date: 01/09/2025\n","*   Version: v1.0\n","\n","**Description**\n","\n","Implementation of Wald & Allen (2007) to estimate Vs,30 on a 30-arc-second grid.\n","Extended to support both Active Shallow Crust (ASC) and Stable Continental Crust (SCC) assumptions. Then generate site model for exposure data.\n","\n","Based on: USGS Global Vs30 methodology (Heath et al. 2020, Wald & Allen 2007)\n","Tectonic classification: Chen et al. (2018), Poggi et al. (2020)\n","\n","**This tool:** Downloads global Vs30 data and creates site models for both ASC and SCC assumptions.\n","\n","**What you need**: Study area coordinate (latitude, longitude), exposure model data\n","\n","**Complete file list**:\n","\n","  üó∫Ô∏è  vs30_grid_morocco_asc.tif - ASC VS30 raster\n","\n","  üó∫Ô∏è  vs30_grid_morocco_scc.tif - SCC VS30 raster\n","\n","  üìä site_model_asc.csv - OpenQuake ASC site model\n","\n","  üìä site_model_scc.csv - OpenQuake SCC site model\n","\n","  üìà buildings_with_vs30_dual.csv - Complete building data\n","\n","  üåê vs30_validation_map_dual.html - Interactive map\n","\n","  üìÑ validation_summary_dual_tectonic.txt - Summary report\n","\n","  üíæ step_*_data.pkl - Intermediate results for session restoration\n"],"metadata":{"id":"GvnEJf0gsgvi"}},{"cell_type":"markdown","source":["## 0 - ENVIRONMENT SETUP AND DATA PREPARATION\n","\n"],"metadata":{"id":"Rlgn6RtPs_G6"}},{"cell_type":"markdown","source":["This section installs required packages and sets up the working environment for processing SRTM elevation data and implementing the Wald & Allen methodology\n","\n","\n","\n","\n"],"metadata":{"id":"UeKzr_7as4Cr"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import requests\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"üì¶ Installing required packages for geospatial processing...\")\n","!pip install rasterio folium geopandas tqdm -q\n","\n","import rasterio\n","from rasterio.features import rasterize\n","from rasterio.transform import from_bounds, rowcol\n","from rasterio.windows import Window\n","from rasterio.crs import CRS\n","import geopandas as gpd\n","from shapely.geometry import Point\n","import folium\n","from google.colab import files\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import json\n","import pickle\n","import os\n","\n","print(\"‚úÖ Environment setup complete!\")\n","print(\"üéØ Ready to implement Wald & Allen (2007) grid-based VS30 estimation\")\n","print(\"üåç Supporting both ASC and SCC tectonic settings\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYqupfo4_d5q","executionInfo":{"status":"ok","timestamp":1752400469009,"user_tz":-60,"elapsed":1876,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"4fa99cae-8d89-46ad-b8f7-c6374704b5cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì¶ Installing required packages for geospatial processing...\n","‚úÖ Environment setup complete!\n","üéØ Ready to implement Wald & Allen (2007) grid-based VS30 estimation\n","üåç Supporting both ASC and SCC tectonic settings\n"]}]},{"cell_type":"code","source":["# Mount Google Drive if not already mounted\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9901KjHsANI","executionInfo":{"status":"ok","timestamp":1752912706002,"user_tz":-60,"elapsed":13026,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"bbf26e65-8da6-44a8-8b6b-945db3e4d31c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Configuration for Google Drive output\n","GDRIVE_OUTPUT_FOLDER = \"/content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\""],"metadata":{"id":"rf8RgJ1MYfUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def setup_output_folder():\n","    \"\"\"\n","    Create the output folder in Google Drive if it doesn't exist\n","    \"\"\"\n","    import os\n","\n","    if not os.path.exists(GDRIVE_OUTPUT_FOLDER):\n","        os.makedirs(GDRIVE_OUTPUT_FOLDER)\n","        print(f\"‚úÖ Created output folder: {GDRIVE_OUTPUT_FOLDER}\")\n","    else:\n","        print(f\"‚úÖ Using existing output folder: {GDRIVE_OUTPUT_FOLDER}\")\n","\n","    return GDRIVE_OUTPUT_FOLDER\n","\n","def save_to_gdrive(filename, description=\"\"):\n","    \"\"\"\n","    Move a file from current directory to Google Drive folder\n","    \"\"\"\n","    import shutil\n","    import os\n","\n","    source_path = filename\n","    destination_path = os.path.join(GDRIVE_OUTPUT_FOLDER, filename)\n","\n","    if os.path.exists(source_path):\n","        shutil.move(source_path, destination_path)\n","        print(f\"üíæ Saved to Google Drive: {filename} {description}\")\n","        return destination_path\n","    else:\n","        print(f\"‚ö†Ô∏è  File not found: {filename}\")\n","        return None\n"],"metadata":{"id":"h8N4BuAv_t8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1 - BASIN DEPTH ESTIMATION FUNCTIONS"],"metadata":{"id":"yEt9GNcqKSnY"}},{"cell_type":"markdown","source":["These functions estimate z1pt0 and z2pt5 from Vs30 using established correlations\n","for both ASC and SCC tectonic settings."],"metadata":{"id":"Lngh_pE3KSWj"}},{"cell_type":"code","source":["def estimate_z1pt0_asc(vs30):\n","    \"\"\"\n","    Estimate z1.0 (depth to 1 km/s) for Active Shallow Crust using Chiou & Youngs (2014) correlation.\n","\n","    Parameters:\n","    vs30 (float or array): VS30 values in m/s\n","\n","    Returns:\n","    z1pt0 (float or array): z1.0 values in km\n","    \"\"\"\n","    # Chiou & Youngs (2014) correlation for ASC regions\n","    # ln(z1.0) = -7.15/4 * ln((vs30^4 + 571^4)/(1360^4 + 571^4))\n","    vs30 = np.array(vs30)\n","\n","    # Apply the correlation\n","    ln_z1pt0 = (-7.15/4.0) * np.log((vs30**4 + 571**4) / (1360**4 + 571**4))\n","    z1pt0 = np.exp(ln_z1pt0)  # Convert from ln to linear units (km)\n","\n","    return z1pt0\n","\n","def estimate_z2pt5_asc(vs30):\n","    \"\"\"\n","    Estimate z2.5 (depth to 2.5 km/s) for Active Shallow Crust using Campbell & Bozorgnia (2014) correlation.\n","\n","    Parameters:\n","    vs30 (float or array): VS30 values in m/s\n","\n","    Returns:\n","    z2pt5 (float or array): z2.5 values in km\n","    \"\"\"\n","    vs30 = np.array(vs30)\n","\n","    # Campbell & Bozorgnia (2014) correlation for ASC regions\n","    # ln(z2.5) = 7.089 - 1.144 * ln(vs30)\n","    ln_z2pt5 = 7.089 - 1.144 * np.log(vs30)\n","    z2pt5 = np.exp(ln_z2pt5)  # Convert from ln to linear units (km)\n","\n","    # Apply reasonable bounds\n","    z2pt5 = np.clip(z2pt5, 0.005, 10.0)  # 5m to 10km\n","\n","    return z2pt5\n","\n","def estimate_z1pt0_scc(vs30):\n","    \"\"\"\n","    Estimate z1.0 (depth to 1 km/s) for Stable Continental Crust.\n","\n","    Note: SCC GMPEs (Atkinson & Boore 2006, Pezeshk et al. 2011) do not use z1pt0\n","    as an input parameter. These models were developed with simpler site\n","    characterization approaches that rely only on VS30 or basic site classes.\n","\n","    Parameters:\n","    vs30 (float or array): VS30 values in m/s\n","\n","    Returns:\n","    str: 0 indicating parameter not used in SCC GMPEs\n","    \"\"\"\n","    z1pt0 = 0\n","    return z1pt0\n","\n","def estimate_z2pt5_scc(vs30):\n","    \"\"\"\n","    Estimate z2.5 (depth to 2.5 km/s) for Stable Continental Crust.\n","\n","    Note: SCC GMPEs (Atkinson & Boore 2006, Pezeshk et al. 2011) do not use z2pt5\n","    as an input parameter. These models use simpler functional forms without\n","    basin depth effects, as the stable continental crust region has less complex\n","    geological structure compared to active tectonic regions.\n","\n","    Parameters:\n","    vs30 (float or array): VS30 values in m/s\n","\n","    Returns:\n","    str: 0 indicating parameter not used in SCC GMPEs\n","    \"\"\"\n","    z2pt5 = 0\n","    return z2pt5"],"metadata":{"id":"A9r93D8QMm4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2 - CREATE SRTM-BASED VS30 GRID WITH TECTONIC SETTINGS"],"metadata":{"id":"dlZK6157tSPD"}},{"cell_type":"markdown","source":["This section downloads SRTM elevation data, calculates topographic slope, and applies Wald & Allen (2007) correlations for both ASC and SCC tectonic settings."],"metadata":{"id":"0aNDBZp0tTsO"}},{"cell_type":"code","source":["def download_srtm_data(bounds):\n","    \"\"\"\n","    Download SRTM elevation data for the study area\n","    Uses Open-Elevation service which provides SRTM-based elevations\n","    \"\"\"\n","    print(\"üåç Accessing SRTM elevation data for Morocco study area...\")\n","\n","    # Create coordinate arrays for the bounding box\n","    west, south, east, north = bounds\n","\n","    # 30 arc-second resolution ‚âà 0.00833 degrees\n","    resolution = 30 / 3600  # 30 arc-seconds in degrees\n","\n","    # Create coordinate grids\n","    lons = np.arange(west, east + resolution, resolution)\n","    lats = np.arange(south, north + resolution, resolution)\n","\n","    print(f\"üìê Grid dimensions: {len(lons)} x {len(lats)} cells\")\n","    print(f\"üîç Resolution: {resolution:.5f} degrees (~900m)\")\n","\n","    return lons, lats, resolution\n","\n","def get_elevation_grid(lons, lats):\n","    \"\"\"\n","    Retrieve elevation values for the entire grid using SRTM data\n","    \"\"\"\n","    print(\"üîÑ Retrieving SRTM elevation data...\")\n","\n","    elevations = np.full((len(lats), len(lons)), np.nan)\n","\n","    # Sample subset for demonstration (every 3rd point to optimize speed)\n","    step = 3\n","    total_points = (len(lats[::step]) * len(lons[::step]))\n","\n","    # Create progress bar\n","    pbar = tqdm(total=total_points, desc=\"üì° Fetching elevations\", unit=\"pts\")\n","    processed = 0\n","\n","    for i, lat in enumerate(lats[::step]):\n","        for j, lon in enumerate(lons[::step]):\n","            try:\n","                url = f\"https://api.open-elevation.com/api/v1/lookup?locations={lat},{lon}\"\n","                response = requests.get(url, timeout=5)\n","                if response.status_code == 200:\n","                    elev = response.json()['results'][0]['elevation']\n","                    elevations[i*step, j*step] = elev\n","\n","                processed += 1\n","                pbar.update(1)\n","\n","            except Exception as e:\n","                processed += 1\n","                pbar.update(1)\n","                continue\n","\n","    pbar.close()\n","\n","    # Fill gaps using interpolation for complete coverage\n","    print(\"üîß Interpolating to fill data gaps...\")\n","    mask = ~np.isnan(elevations)\n","    if np.sum(mask) > 0:\n","        from scipy.interpolate import griddata\n","        points = np.column_stack((np.repeat(lats[:, None], len(lons), axis=1)[mask],\n","                                np.repeat(lons[None, :], len(lats), axis=0)[mask]))\n","        values = elevations[mask]\n","\n","        # Create full coordinate grid for interpolation\n","        lon_grid, lat_grid = np.meshgrid(lons, lats)\n","        elevations = griddata(points, values, (lat_grid, lon_grid), method='linear')\n","\n","        # Fill remaining NaNs with median elevation\n","        elevations[np.isnan(elevations)] = np.nanmedian(elevations)\n","\n","    print(f\"‚úÖ Elevation grid complete. Range: {np.nanmin(elevations):.0f} - {np.nanmax(elevations):.0f} m\")\n","    return elevations\n","\n","def calculate_slope_grid(elevations, resolution):\n","    \"\"\"\n","    Calculate topographic slope from elevation grid using gradient method\n","    This implements the slope calculation equivalent to GMT's grdgradient\n","    \"\"\"\n","    print(\"üìê Calculating topographic slope grid...\")\n","\n","    # Convert resolution from degrees to meters (approximate for Morocco latitude)\n","    meters_per_degree = 111320  # at equator\n","    resolution_meters = resolution * meters_per_degree\n","\n","    # Calculate gradients in both directions\n","    dy, dx = np.gradient(elevations, resolution_meters)\n","\n","    # Calculate slope magnitude (rise over run)\n","    slope = np.sqrt(dx**2 + dy**2)\n","\n","    # Convert to m/m units as used in Wald & Allen (2007)\n","    slope = slope.astype(np.float32)\n","\n","    print(f\"‚úÖ Slope calculation complete. Range: {np.nanmin(slope):.6f} - {np.nanmax(slope):.6f} m/m\")\n","\n","    return slope\n","\n","def apply_wald_allen_correlations_dual(slope_grid, lons, lats):\n","    \"\"\"\n","    Apply Wald & Allen (2007) correlations for both ASC and SCC tectonic settings.\n","    Morocco classification based on Chen et al. (2018) and Poggi et al. (2020).\n","\n","    Returns both ASC and SCC VS30 grids for comparison studies.\n","    \"\"\"\n","    print(\"üèîÔ∏è  Applying Wald & Allen (2007) correlations for dual tectonic settings...\")\n","    print(\"üìä ASC: Active Shallow Crust (near Atlas Mountains)\")\n","    print(\"üìä SCC: Stable Continental Crust (interior plains)\")\n","\n","    # Table 1 from Wald & Allen (2007) - Active Shallow Crust correlations\n","    asc_correlations = [\n","        ('E', (150, 180), (0, 2.0e-5)),\n","        ('D1', (180, 240), (2.0e-5, 2.0e-3)),\n","        ('D2', (240, 300), (2.0e-3, 4.0e-3)),\n","        ('D3', (300, 360), (4.0e-3, 7.2e-3)),\n","        ('C1', (360, 490), (7.2e-3, 0.013)),\n","        ('C2', (490, 620), (0.013, 0.018)),\n","        ('C3', (620, 760), (0.018, 0.025)),\n","        ('B', (760, 1500), (0.025, 1.0))\n","    ]\n","\n","    # Table 2 from Wald & Allen (2007) - Stable Continental correlations\n","    scc_correlations = [\n","        ('E', (180, 240), (0, 2.0e-5)),\n","        ('D1', (240, 300), (2.0e-5, 2.0e-3)),\n","        ('D2', (300, 360), (2.0e-3, 4.0e-3)),\n","        ('D3', (360, 490), (4.0e-3, 7.2e-3)),\n","        ('C1', (490, 620), (7.2e-3, 0.013)),\n","        ('C2', (620, 760), (0.013, 0.018)),\n","        ('C3', (760, 900), (0.018, 0.025)),\n","        ('B', (900, 1200), (0.025, 1.0))\n","    ]\n","\n","    # Initialize output grids\n","    vs30_asc_grid = np.full_like(slope_grid, np.nan)\n","    vs30_scc_grid = np.full_like(slope_grid, np.nan)\n","    class_asc_grid = np.full(slope_grid.shape, '', dtype='U2')\n","    class_scc_grid = np.full(slope_grid.shape, '', dtype='U2')\n","\n","    # Apply ASC correlations\n","    print(\"üåã Processing Active Shallow Crust correlations...\")\n","    for class_name, (vs30_min, vs30_max), (slope_min, slope_max) in asc_correlations:\n","        mask = (slope_grid >= slope_min) & (slope_grid < slope_max)\n","        if np.sum(mask) > 0:\n","            slope_normalized = (slope_grid[mask] - slope_min) / (slope_max - slope_min)\n","            vs30_interpolated = vs30_min + slope_normalized * (vs30_max - vs30_min)\n","            vs30_asc_grid[mask] = vs30_interpolated\n","            class_asc_grid[mask] = class_name\n","\n","    # Apply SCC correlations\n","    print(\"üèîÔ∏è  Processing Stable Continental Crust correlations...\")\n","    for class_name, (vs30_min, vs30_max), (slope_min, slope_max) in scc_correlations:\n","        mask = (slope_grid >= slope_min) & (slope_grid < slope_max)\n","        if np.sum(mask) > 0:\n","            slope_normalized = (slope_grid[mask] - slope_min) / (slope_max - slope_min)\n","            vs30_interpolated = vs30_min + slope_normalized * (vs30_max - vs30_min)\n","            vs30_scc_grid[mask] = vs30_interpolated\n","            class_scc_grid[mask] = class_name\n","\n","    # Handle any remaining NaN values for both grids\n","    for vs30_grid, class_grid, tectonic_type in [(vs30_asc_grid, class_asc_grid, \"ASC\"),\n","                                                 (vs30_scc_grid, class_scc_grid, \"SCC\")]:\n","        nan_mask = np.isnan(vs30_grid)\n","        if np.sum(nan_mask) > 0:\n","            max_vs30 = 1200 if tectonic_type == \"ASC\" else 1000\n","            vs30_grid[nan_mask] = max_vs30\n","            class_grid[nan_mask] = 'B'\n","\n","    print(f\"‚úÖ Dual VS30 correlations complete:\")\n","    print(f\"üìä ASC VS30 range: {np.nanmin(vs30_asc_grid):.0f} - {np.nanmax(vs30_asc_grid):.0f} m/s\")\n","    print(f\"üìä SCC VS30 range: {np.nanmin(vs30_scc_grid):.0f} - {np.nanmax(vs30_scc_grid):.0f} m/s\")\n","\n","    # Print class distributions\n","    for vs30_grid, class_grid, tectonic_type in [(vs30_asc_grid, class_asc_grid, \"ASC\"),\n","                                                 (vs30_scc_grid, class_scc_grid, \"SCC\")]:\n","        unique_classes, counts = np.unique(class_grid, return_counts=True)\n","        print(f\"üèóÔ∏è  NEHRP class distribution ({tectonic_type}):\")\n","        for cls, count in zip(unique_classes, counts):\n","            pct = count / np.sum(counts) * 100\n","            print(f\"   Class {cls}: {count} cells ({pct:.1f}%)\")\n","\n","    return vs30_asc_grid, vs30_scc_grid, class_asc_grid, class_scc_grid\n","\n","def create_vs30_rasters(vs30_asc_grid, vs30_scc_grid, lons, lats, bounds):\n","    \"\"\"\n","    Create GeoTIFF raster files for both ASC and SCC VS30 grids and save to Google Drive\n","    Returns the local file paths for immediate use in the same session\n","    \"\"\"\n","    print(\"üíæ Creating VS30 GeoTIFF rasters for both tectonic settings...\")\n","\n","    # Define the transform (maps pixel coordinates to geographic coordinates)\n","    west, south, east, north = bounds\n","    transform = from_bounds(west, south, east, north, vs30_asc_grid.shape[1], vs30_asc_grid.shape[0])\n","\n","    local_raster_files = []\n","    gdrive_raster_files = []\n","\n","    # Create ASC raster\n","    asc_file = 'vs30_grid_morocco_asc.tif'\n","    with rasterio.open(\n","        asc_file, 'w', driver='GTiff',\n","        height=vs30_asc_grid.shape[0], width=vs30_asc_grid.shape[1], count=1,\n","        dtype=vs30_asc_grid.dtype, crs=CRS.from_epsg(4326), transform=transform, compress='lzw'\n","    ) as dst:\n","        dst.write(vs30_asc_grid, 1)\n","        dst.set_band_description(1, 'VS30 (m/s) from Wald & Allen 2007 - Active Shallow Crust')\n","\n","    local_raster_files.append(asc_file)\n","    print(f\"‚úÖ Created local ASC raster: {asc_file}\")\n","\n","    # Create SCC raster\n","    scc_file = 'vs30_grid_morocco_scc.tif'\n","    with rasterio.open(\n","        scc_file, 'w', driver='GTiff',\n","        height=vs30_scc_grid.shape[0], width=vs30_scc_grid.shape[1], count=1,\n","        dtype=vs30_scc_grid.dtype, crs=CRS.from_epsg(4326), transform=transform, compress='lzw'\n","    ) as dst:\n","        dst.write(vs30_scc_grid, 1)\n","        dst.set_band_description(1, 'VS30 (m/s) from Wald & Allen 2007 - Stable Continental Crust')\n","\n","    local_raster_files.append(scc_file)\n","    print(f\"‚úÖ Created local SCC raster: {scc_file}\")\n","\n","    # Now save copies to Google Drive\n","    import shutil\n","    import os\n","\n","    for raster_file in local_raster_files:\n","        try:\n","            destination_path = os.path.join(GDRIVE_OUTPUT_FOLDER, raster_file)\n","            shutil.copy2(raster_file, destination_path)  # Use copy2 instead of move to keep local files\n","            gdrive_raster_files.append(destination_path)\n","            tectonic_type = \"ASC\" if \"asc\" in raster_file else \"SCC\"\n","            print(f\"üíæ Saved to Google Drive: {raster_file} ({tectonic_type} VS30 raster)\")\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Warning: Could not save {raster_file} to Google Drive: {e}\")\n","            gdrive_raster_files.append(None)\n","\n","    print(f\"‚úÖ VS30 rasters created locally and saved to Google Drive\")\n","    return local_raster_files, gdrive_raster_files"],"metadata":{"id":"WM51Asy7KpVW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3 - GRID CELL ASSIGNMENT FOR BUILDINGS WITH DUAL TECTONIC SETTINGS"],"metadata":{"id":"oBd63PFitgpo"}},{"cell_type":"markdown","source":["This section loads exposure data and assigns VS30 values from both grids,\n","including basin depth estimates for both ASC and SCC assumptions."],"metadata":{"id":"xU-CkQJrtaKK"}},{"cell_type":"code","source":["def load_exposure_data():\n","    \"\"\"\n","    Load building exposure data and validate coordinates\n","    \"\"\"\n","    print(\"üìã Loading building exposure data...\")\n","\n","    # Upload CSV file\n","    uploaded = files.upload()\n","    filename = list(uploaded.keys())[0]\n","\n","    # Load and validate data\n","    df = pd.read_csv(filename)\n","\n","    required_cols = ['id', 'lat', 'lon']\n","    missing_cols = [col for col in required_cols if col not in df.columns]\n","    if missing_cols:\n","        raise ValueError(f\"Missing required columns: {missing_cols}\")\n","\n","    # Clean and validate coordinates\n","    df = df.dropna(subset=['lat', 'lon'])\n","    df = df[(df['lat'].between(-90, 90)) & (df['lon'].between(-180, 180))]\n","\n","    print(f\"‚úÖ Loaded {len(df)} valid building locations\")\n","    print(f\"üìç Coordinate bounds: Lat [{df['lat'].min():.3f}, {df['lat'].max():.3f}], \"\n","          f\"Lon [{df['lon'].min():.3f}, {df['lon'].max():.3f}]\")\n","\n","    return df\n","\n","def assign_vs30_dual_tectonic(buildings_df, asc_raster_path, scc_raster_path, resolution):\n","    \"\"\"\n","    Assign VS30 values to building locations from both ASC and SCC grids.\n","    Calculate basin depths (z1pt0, z2pt5) for both tectonic settings.\n","    \"\"\"\n","    print(\"üéØ Assigning VS30 values for both ASC and SCC tectonic settings...\")\n","\n","    # Initialize result arrays\n","    results = {\n","        'asc': {'vs30': [], 'z1pt0': [], 'z2pt5': [], 'edge_flags': [], 'grid_distances': []},\n","        'scc': {'vs30': [], 'z1pt0': [], 'z2pt5': [], 'edge_flags': [], 'grid_distances': []}\n","    }\n","\n","    # Process ASC raster\n","    print(\"üåã Processing Active Shallow Crust assignments...\")\n","    with rasterio.open(asc_raster_path) as src:\n","        for idx, row in buildings_df.iterrows():\n","            lat, lon = row['lat'], row['lon']\n","\n","            try:\n","                raster_x, raster_y = ~src.transform * (lon, lat)\n","                col, row_idx = int(raster_x), int(raster_y)\n","\n","                if 0 <= col < src.width and 0 <= row_idx < src.height:\n","                    vs30_value = src.read(1)[row_idx, col]\n","                    results['asc']['vs30'].append(vs30_value)\n","\n","                    # Calculate basin depths for ASC\n","                    z1pt0_asc = estimate_z1pt0_asc(vs30_value)\n","                    z2pt5_asc = estimate_z2pt5_asc(vs30_value)\n","                    results['asc']['z1pt0'].append(z1pt0_asc)\n","                    results['asc']['z2pt5'].append(z2pt5_asc)\n","\n","                    # Calculate distance to cell edge\n","                    cell_center_x = col + 0.5\n","                    cell_center_y = row_idx + 0.5\n","                    dist_x = abs(raster_x - cell_center_x)\n","                    dist_y = abs(raster_y - cell_center_y)\n","                    max_distance = max(dist_x, dist_y)\n","                    edge_flag = max_distance > 0.25\n","\n","                    results['asc']['edge_flags'].append(edge_flag)\n","                    results['asc']['grid_distances'].append(max_distance)\n","                else:\n","                    # Outside bounds\n","                    results['asc']['vs30'].append(np.nan)\n","                    results['asc']['z1pt0'].append(np.nan)\n","                    results['asc']['z2pt5'].append(np.nan)\n","                    results['asc']['edge_flags'].append(True)\n","                    results['asc']['grid_distances'].append(np.nan)\n","            except:\n","                results['asc']['vs30'].append(np.nan)\n","                results['asc']['z1pt0'].append(np.nan)\n","                results['asc']['z2pt5'].append(np.nan)\n","                results['asc']['edge_flags'].append(True)\n","                results['asc']['grid_distances'].append(np.nan)\n","\n","    # Process SCC raster\n","    print(\"üèîÔ∏è  Processing Stable Continental Crust assignments...\")\n","    with rasterio.open(scc_raster_path) as src:\n","        for idx, row in buildings_df.iterrows():\n","            lat, lon = row['lat'], row['lon']\n","\n","            try:\n","                raster_x, raster_y = ~src.transform * (lon, lat)\n","                col, row_idx = int(raster_x), int(raster_y)\n","\n","                if 0 <= col < src.width and 0 <= row_idx < src.height:\n","                    vs30_value = src.read(1)[row_idx, col]\n","                    results['scc']['vs30'].append(vs30_value)\n","\n","                    # Calculate basin depths for SCC\n","                    z1pt0_scc = estimate_z1pt0_scc(vs30_value)\n","                    z2pt5_scc = estimate_z2pt5_scc(vs30_value)\n","                    results['scc']['z1pt0'].append(z1pt0_scc)\n","                    results['scc']['z2pt5'].append(z2pt5_scc)\n","\n","                    # Calculate distance to cell edge (same as ASC)\n","                    cell_center_x = col + 0.5\n","                    cell_center_y = row_idx + 0.5\n","                    dist_x = abs(raster_x - cell_center_x)\n","                    dist_y = abs(raster_y - cell_center_y)\n","                    max_distance = max(dist_x, dist_y)\n","                    edge_flag = max_distance > 0.25\n","\n","                    results['scc']['edge_flags'].append(edge_flag)\n","                    results['scc']['grid_distances'].append(max_distance)\n","                else:\n","                    # Outside bounds\n","                    results['scc']['vs30'].append(np.nan)\n","                    results['scc']['z1pt0'].append(np.nan)\n","                    results['scc']['z2pt5'].append(np.nan)\n","                    results['scc']['edge_flags'].append(True)\n","                    results['scc']['grid_distances'].append(np.nan)\n","            except:\n","                results['scc']['vs30'].append(np.nan)\n","                results['scc']['z1pt0'].append(np.nan)\n","                results['scc']['z2pt5'].append(np.nan)\n","                results['scc']['edge_flags'].append(True)\n","                results['scc']['grid_distances'].append(np.nan)\n","\n","    # Add results to dataframe with appropriate suffixes\n","    buildings_df['vs30_asc'] = results['asc']['vs30']\n","    buildings_df['z1pt0_asc'] = results['asc']['z1pt0']\n","    buildings_df['z2pt5_asc'] = results['asc']['z2pt5']\n","    buildings_df['edge_flag_asc'] = results['asc']['edge_flags']\n","\n","    buildings_df['vs30_scc'] = results['scc']['vs30']\n","    buildings_df['z1pt0_scc'] = results['scc']['z1pt0']\n","    buildings_df['z2pt5_scc'] = results['scc']['z2pt5']\n","    buildings_df['edge_flag_scc'] = results['scc']['edge_flags']\n","\n","    # Add NEHRP classifications\n","    def classify_vs30(vs30):\n","        if pd.isna(vs30):\n","            return 'Unknown'\n","        elif vs30 >= 760:\n","            return 'B'\n","        elif vs30 >= 360:\n","            return 'C'\n","        elif vs30 >= 180:\n","            return 'D'\n","        else:\n","            return 'E'\n","\n","    buildings_df['nehrp_class_asc'] = buildings_df['vs30_asc'].apply(classify_vs30)\n","    buildings_df['nehrp_class_scc'] = buildings_df['vs30_scc'].apply(classify_vs30)\n","\n","    # Quality control summary\n","    valid_asc = buildings_df['vs30_asc'].notna().sum()\n","    valid_scc = buildings_df['vs30_scc'].notna().sum()\n","    edge_asc = buildings_df['edge_flag_asc'].sum()\n","    edge_scc = buildings_df['edge_flag_scc'].sum()\n","\n","    print(f\"‚úÖ Dual tectonic VS30 assignment complete:\")\n","    print(f\"   ASC: {valid_asc}/{len(buildings_df)} buildings assigned, {edge_asc} edge-flagged\")\n","    print(f\"   SCC: {valid_scc}/{len(buildings_df)} buildings assigned, {edge_scc} edge-flagged\")\n","    print(f\"   ASC VS30 range: {buildings_df['vs30_asc'].min():.0f} - {buildings_df['vs30_asc'].max():.0f} m/s\")\n","    print(f\"   SCC VS30 range: {buildings_df['vs30_scc'].min():.0f} - {buildings_df['vs30_scc'].max():.0f} m/s\")\n","\n","    return buildings_df\n"],"metadata":{"id":"nKxuZXrUK1kW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4 - OPENQUAKE OUTPUT AND VALIDATION SUMMARY"],"metadata":{"id":"HPnqWz7qtvo2"}},{"cell_type":"markdown","source":["This section creates OpenQuake Engine compatible site models for both tectonic settings\n","and generates comprehensive validation reports."],"metadata":{"id":"hzJ93n_AuMkf"}},{"cell_type":"code","source":["def create_openquake_site_models_dual(buildings_df):\n","    \"\"\"\n","    Generate OpenQuake Engine compatible site models for both ASC and SCC settings.\n","    Creates separate CSV files with the required structure: ID, lat, lon, vs30, z1pt0, z2pt5\n","    Saves files to Google Drive.\n","    \"\"\"\n","    print(\"üè≠ Creating OpenQuake Engine site models for both tectonic settings...\")\n","\n","    # Filter valid assignments for each setting\n","    valid_asc = buildings_df.dropna(subset=['vs30_asc', 'z1pt0_asc', 'z2pt5_asc'])\n","    valid_scc = buildings_df.dropna(subset=['vs30_scc', 'z1pt0_scc', 'z2pt5_scc'])\n","\n","    # Create ASC site model with required structure\n","    site_model_asc = pd.DataFrame({\n","        'ID': valid_asc['id'],\n","        'lat': valid_asc['lat'],\n","        'lon': valid_asc['lon'],\n","        'vs30': valid_asc['vs30_asc'].round(0).astype(int),\n","        'z1pt0': valid_asc['z1pt0_asc'].round(6),  # km, 6 decimal places\n","        'z2pt5': valid_asc['z2pt5_asc'].round(6)   # km, 6 decimal places\n","    })\n","\n","    # Create SCC site model with required structure\n","    site_model_scc = pd.DataFrame({\n","        'ID': valid_scc['id'],\n","        'lat': valid_scc['lat'],\n","        'lon': valid_scc['lon'],\n","        'vs30': valid_scc['vs30_scc'].round(0).astype(int),\n","        'z1pt0': valid_scc['z1pt0_scc'].round(6),  # km, 6 decimal places\n","        'z2pt5': valid_scc['z2pt5_scc'].round(6)   # km, 6 decimal places\n","    })\n","\n","    # Save site models locally first\n","    asc_file = 'site_model_asc.csv'\n","    scc_file = 'site_model_scc.csv'\n","\n","    site_model_asc.to_csv(asc_file, index=False)\n","    site_model_scc.to_csv(scc_file, index=False)\n","\n","    # Save to Google Drive\n","    save_to_gdrive(asc_file, f\"(ASC OpenQuake site model - {len(site_model_asc)} sites)\")\n","    save_to_gdrive(scc_file, f\"(SCC OpenQuake site model - {len(site_model_scc)} sites)\")\n","\n","    print(f\"‚úÖ OpenQuake site models saved to Google Drive:\")\n","    print(f\"   ASC model: '{asc_file}' ({len(site_model_asc)} sites)\")\n","    print(f\"   SCC model: '{scc_file}' ({len(site_model_scc)} sites)\")\n","\n","    # Display sample data\n","    print(f\"\\nüìä Sample ASC site model data:\")\n","    print(site_model_asc.head())\n","    print(f\"\\nüìä Sample SCC site model data:\")\n","    print(site_model_scc.head())\n","\n","    return site_model_asc, site_model_scc, asc_file, scc_file\n","\n","def create_validation_summary_dual(buildings_df):\n","    \"\"\"\n","    Create comprehensive summary statistics and quality control report for both tectonic settings\n","    Save to Google Drive.\n","    \"\"\"\n","    print(\"üìä Generating dual tectonic setting validation summary...\")\n","\n","    valid_asc = buildings_df.dropna(subset=['vs30_asc'])\n","    valid_scc = buildings_df.dropna(subset=['vs30_scc'])\n","\n","    summary_report = f\"\"\"\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    DUAL TECTONIC SETTING VS30 ESTIMATION VALIDATION SUMMARY\n","    Morocco Earthquake Study - Wald & Allen (2007) Implementation\n","    Active Shallow Crust (ASC) vs Stable Continental Crust (SCC) Comparison\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","\n","    üéØ Methodology:\n","    ‚Ä¢ Grid Resolution: 30 arc-second (~900m) SRTM-based\n","    ‚Ä¢ Tectonic Classifications:\n","      - ASC: Active Shallow Crust (Wald & Allen 2007, Table 1)\n","      - SCC: Stable Continental Crust (Wald & Allen 2007, Table 2)\n","    ‚Ä¢ Basin Depth Correlations:\n","      - ASC z1.0: Chiou & Youngs (2014), z2.5: Campbell & Bozorgnia (2014)\n","      - SCC z1.0: Modified CY14 (65% scaling), z2.5: Modified CB14 (70% scaling)\n","    ‚Ä¢ Coverage: {len(buildings_df)} building locations\n","    ‚Ä¢ Valid ASC Assignments: {len(valid_asc)} ({len(valid_asc)/len(buildings_df)*100:.1f}%)\n","    ‚Ä¢ Valid SCC Assignments: {len(valid_scc)} ({len(valid_scc)/len(buildings_df)*100:.1f}%)\n","\n","    üìä ACTIVE SHALLOW CRUST (ASC) RESULTS:\n","    ‚Ä¢ Mean VS30: {valid_asc['vs30_asc'].mean():.0f} m/s\n","    ‚Ä¢ Median VS30: {valid_asc['vs30_asc'].median():.0f} m/s\n","    ‚Ä¢ Standard Deviation: {valid_asc['vs30_asc'].std():.0f} m/s\n","    ‚Ä¢ Range: {valid_asc['vs30_asc'].min():.0f} - {valid_asc['vs30_asc'].max():.0f} m/s\n","    ‚Ä¢ Mean z1.0: {valid_asc['z1pt0_asc'].mean():.3f} km\n","    ‚Ä¢ Mean z2.5: {valid_asc['z2pt5_asc'].mean():.3f} km\n","\n","    üìä STABLE CONTINENTAL CRUST (SCC) RESULTS:\n","    ‚Ä¢ Mean VS30: {valid_scc['vs30_scc'].mean():.0f} m/s\n","    ‚Ä¢ Median VS30: {valid_scc['vs30_scc'].median():.0f} m/s\n","    ‚Ä¢ Standard Deviation: {valid_scc['vs30_scc'].std():.0f} m/s\n","    ‚Ä¢ Range: {valid_scc['vs30_scc'].min():.0f} - {valid_scc['vs30_scc'].max():.0f} m/s\n","    ‚Ä¢ Mean z1.0: {valid_scc['z1pt0_scc'].mean():.3f} km\n","    ‚Ä¢ Mean z2.5: {valid_scc['z2pt5_scc'].mean():.3f} km\n","\n","    üîÑ TECTONIC SETTING COMPARISON:\n","    ‚Ä¢ VS30 Difference (ASC-SCC): Mean = {(valid_asc['vs30_asc'] - valid_scc['vs30_scc']).mean():.0f} m/s\n","    ‚Ä¢ z1.0 Difference (ASC-SCC): Mean = {(valid_asc['z1pt0_asc'] - valid_scc['z1pt0_scc']).mean():.3f} km\n","    ‚Ä¢ z2.5 Difference (ASC-SCC): Mean = {(valid_asc['z2pt5_asc'] - valid_scc['z2pt5_scc']).mean():.3f} km\n","\n","    üèóÔ∏è  NEHRP SITE CLASSIFICATION COMPARISON:\n","    \"\"\"\n","\n","    # Add ASC class distribution\n","    asc_class_counts = valid_asc['nehrp_class_asc'].value_counts().sort_index()\n","    summary_report += \"\\n    ASC Classification:\\n\"\n","    for site_class, count in asc_class_counts.items():\n","        pct = count / len(valid_asc) * 100\n","        summary_report += f\"      ‚Ä¢ Class {site_class}: {count} sites ({pct:.1f}%)\\n\"\n","\n","    # Add SCC class distribution\n","    scc_class_counts = valid_scc['nehrp_class_scc'].value_counts().sort_index()\n","    summary_report += \"\\n    SCC Classification:\\n\"\n","    for site_class, count in scc_class_counts.items():\n","        pct = count / len(valid_scc) * 100\n","        summary_report += f\"      ‚Ä¢ Class {site_class}: {count} sites ({pct:.1f}%)\\n\"\n","\n","    # Add edge effect analysis\n","    edge_asc = buildings_df['edge_flag_asc'].sum()\n","    edge_scc = buildings_df['edge_flag_scc'].sum()\n","\n","    summary_report += f\"\"\"\n","    ‚ö†Ô∏è  EDGE EFFECT ANALYSIS:\n","    ‚Ä¢ ASC buildings near grid edges: {edge_asc} ({edge_asc/len(buildings_df)*100:.1f}%)\n","    ‚Ä¢ SCC buildings near grid edges: {edge_scc} ({edge_scc/len(buildings_df)*100:.1f}%)\n","    ‚Ä¢ Recommendation: Consider sensitivity analysis for edge-flagged sites\n","    ‚Ä¢ Grid distance range: {buildings_df[['vs30_asc', 'vs30_scc']].notna().any(axis=1).sum()} sites processed\n","\n","    üéØ GMPE COMPATIBILITY:\n","    ‚Ä¢ ASC Compatible GMPEs: Chiou & Youngs (2014), Akkar et al. (2014)\n","    ‚Ä¢ SCC Compatible GMPEs: Atkinson & Boore (2006), Pezeshk et al. (2011)\n","    ‚Ä¢ Basin depth terms included for both settings\n","    ‚Ä¢ Ready for OpenQuake Engine hazard calculations\n","\n","    ‚úÖ VALIDATION STUDY READINESS:\n","    ‚Ä¢ Dual tectonic setting approach enables sensitivity analysis\n","    ‚Ä¢ Consistent methodology applied across both settings\n","    ‚Ä¢ Basin depth estimates included for realistic ground motion prediction\n","    ‚Ä¢ Edge effects identified for quality control\n","    ‚Ä¢ Ready for damage correlation analysis with both ASC and SCC assumptions\n","\n","    üìÅ OUTPUT FILES (Saved to Google Drive):\n","    ‚Ä¢ vs30_grid_morocco_asc.tif: QGIS-compatible ASC VS30 raster\n","    ‚Ä¢ vs30_grid_morocco_scc.tif: QGIS-compatible SCC VS30 raster\n","    ‚Ä¢ site_model_asc.csv: OpenQuake ASC site model (ID, lat, lon, vs30, z1pt0, z2pt5)\n","    ‚Ä¢ site_model_scc.csv: OpenQuake SCC site model (ID, lat, lon, vs30, z1pt0, z2pt5)\n","    ‚Ä¢ buildings_with_vs30_dual.csv: Complete building data with dual assignments\n","    ‚Ä¢ vs30_validation_map_dual.html: Interactive comparison map\n","\n","    ‚ö° RECOMMENDATION FOR HAZARD ANALYSIS:\n","    Morocco's location near the Africa-Eurasia plate boundary suggests ASC conditions\n","    are more appropriate for the Atlas Mountains region, while SCC may apply to\n","    interior areas. Consider using:\n","    ‚Ä¢ ASC for sites within 100km of active faulting/Atlas Mountains\n","    ‚Ä¢ SCC for sites in stable interior regions\n","    ‚Ä¢ Both models for sensitivity analysis and uncertainty quantification\n","\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    \"\"\"\n","\n","    print(summary_report)\n","\n","    # Save detailed building data with both tectonic settings locally first\n","    buildings_df.to_csv('buildings_with_vs30_dual.csv', index=False)\n","    save_to_gdrive('buildings_with_vs30_dual.csv', \"(Complete building data with dual tectonic assignments)\")\n","\n","    # Save summary report locally first\n","    with open('validation_summary_dual_tectonic.txt', 'w') as f:\n","        f.write(summary_report)\n","    save_to_gdrive('validation_summary_dual_tectonic.txt', \"(Validation summary report)\")\n","\n","    print(\"‚úÖ Dual tectonic validation summary complete and saved to Google Drive!\")\n","    return summary_report\n","\n","def create_interactive_map_dual(buildings_df, bounds):\n","    \"\"\"\n","    Create interactive map comparing ASC and SCC VS30 assignments\n","    \"\"\"\n","    print(\"üó∫Ô∏è  Creating interactive dual tectonic setting comparison map...\")\n","\n","    # Calculate map center\n","    center_lat = (bounds[1] + bounds[3]) / 2\n","    center_lon = (bounds[0] + bounds[2]) / 2\n","\n","    # Create base map with two layers\n","    m = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n","\n","    # Color mapping for VS30 values\n","    def get_color_asc(vs30, edge_flag):\n","        if pd.isna(vs30):\n","            return 'gray'\n","        elif edge_flag:\n","            return 'orange'\n","        elif vs30 >= 760:\n","            return 'darkgreen'\n","        elif vs30 >= 360:\n","            return 'green'\n","        elif vs30 >= 180:\n","            return 'yellow'\n","        else:\n","            return 'red'\n","\n","    def get_color_scc(vs30, edge_flag):\n","        if pd.isna(vs30):\n","            return 'gray'\n","        elif edge_flag:\n","            return 'purple'\n","        elif vs30 >= 760:\n","            return 'darkblue'\n","        elif vs30 >= 360:\n","            return 'blue'\n","        elif vs30 >= 180:\n","            return 'lightblue'\n","        else:\n","            return 'pink'\n","\n","    # Create feature groups for each tectonic setting\n","    asc_group = folium.FeatureGroup(name='ASC Sites', show=True)\n","    scc_group = folium.FeatureGroup(name='SCC Sites', show=False)\n","\n","    # Add ASC markers\n","    for idx, row in buildings_df.iterrows():\n","        if pd.notna(row['vs30_asc']):\n","            popup_text = (f\"ID: {row['id']}<br>\"\n","                         f\"ASC VS30: {row['vs30_asc']:.0f} m/s<br>\"\n","                         f\"ASC NEHRP: {row['nehrp_class_asc']}<br>\"\n","                         f\"ASC z1.0: {row['z1pt0_asc']:.3f} km<br>\"\n","                         f\"ASC z2.5: {row['z2pt5_asc']:.3f} km<br>\"\n","                         f\"Edge Flag: {'Yes' if row['edge_flag_asc'] else 'No'}\")\n","\n","            folium.CircleMarker(\n","                location=[row['lat'], row['lon']],\n","                radius=4,\n","                popup=popup_text,\n","                color=get_color_asc(row['vs30_asc'], row['edge_flag_asc']),\n","                fill=True,\n","                fillColor=get_color_asc(row['vs30_asc'], row['edge_flag_asc']),\n","                fillOpacity=0.7\n","            ).add_to(asc_group)\n","\n","    # Add SCC markers\n","    for idx, row in buildings_df.iterrows():\n","        if pd.notna(row['vs30_scc']):\n","            popup_text = (f\"ID: {row['id']}<br>\"\n","                         f\"SCC VS30: {row['vs30_scc']:.0f} m/s<br>\"\n","                         f\"SCC NEHRP: {row['nehrp_class_scc']}<br>\"\n","                         f\"SCC z1.0: {row['z1pt0_scc']:.3f} km<br>\"\n","                         f\"SCC z2.5: {row['z2pt5_scc']:.3f} km<br>\"\n","                         f\"Edge Flag: {'Yes' if row['edge_flag_scc'] else 'No'}\")\n","\n","            folium.CircleMarker(\n","                location=[row['lat'], row['lon']],\n","                radius=4,\n","                popup=popup_text,\n","                color=get_color_scc(row['vs30_scc'], row['edge_flag_scc']),\n","                fill=True,\n","                fillColor=get_color_scc(row['vs30_scc'], row['edge_flag_scc']),\n","                fillOpacity=0.7\n","            ).add_to(scc_group)\n","\n","    # Add feature groups to map\n","    asc_group.add_to(m)\n","    scc_group.add_to(m)\n","\n","    # Add layer control\n","    folium.LayerControl().add_to(m)\n","\n","    # Add comprehensive legend\n","    legend_html = '''\n","    <div style=\"position: fixed;\n","                bottom: 50px; left: 50px; width: 320px; height: 280px;\n","                background-color: white; border:2px solid grey; z-index:9999;\n","                font-size:11px; padding: 10px\">\n","    <p><strong>VS30 Dual Tectonic Setting Comparison</strong></p>\n","    <p><strong>ASC (Active Shallow Crust):</strong></p>\n","    <p><i class=\"fa fa-circle\" style=\"color:darkgreen\"></i> B: VS30 ‚â• 760 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:green\"></i> C: 360-760 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:yellow\"></i> D: 180-360 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:red\"></i> E: VS30 < 180 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:orange\"></i> Edge-flagged</p>\n","\n","    <p><strong>SCC (Stable Continental Crust):</strong></p>\n","    <p><i class=\"fa fa-circle\" style=\"color:darkblue\"></i> B: VS30 ‚â• 760 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:blue\"></i> C: 360-760 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:lightblue\"></i> D: 180-360 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:pink\"></i> E: VS30 < 180 m/s</p>\n","    <p><i class=\"fa fa-circle\" style=\"color:purple\"></i> Edge-flagged</p>\n","\n","    <p><small>Wald & Allen (2007) - 30 arc-sec grid<br>\n","    Use layer control to switch between ASC/SCC</small></p>\n","    </div>\n","    '''\n","    m.get_root().html.add_child(folium.Element(legend_html))\n","\n","    # Save map\n","    map_file = 'vs30_validation_map_dual.html'\n","    m.save(map_file)\n","\n","    print(f\"‚úÖ Interactive dual tectonic map saved as '{map_file}'\")\n","    return m"],"metadata":{"id":"pEmE1-eDyLup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5 - MAIN EXECUTION WORKFLOW WITH DUAL TECTONIC SETTINGS"],"metadata":{"id":"P42kXoPxueYP"}},{"cell_type":"markdown","source":["These functions allow you to run each step separately with all data saved to Google Drive\n","for persistence across sessions. Step 1 results are saved and can be loaded for subsequent steps."],"metadata":{"id":"ljGlm7lUueuv"}},{"cell_type":"code","source":["def save_step_results_to_gdrive(step_number, results_dict):\n","    \"\"\"\n","    Save step results to Google Drive for persistence across sessions\n","    \"\"\"\n","    # Save as JSON for human-readable metadata\n","    json_filename = f\"step_{step_number}_results.json\"\n","\n","    # Create a JSON-serializable version of results\n","    json_results = {}\n","    for key, value in results_dict.items():\n","        if isinstance(value, np.ndarray):\n","            json_results[key] = {\"type\": \"numpy_array\", \"shape\": value.shape, \"dtype\": str(value.dtype)}\n","        elif isinstance(value, (list, tuple)):\n","            json_results[key] = {\"type\": type(value).__name__, \"value\": value}\n","        elif isinstance(value, (int, float, str)):\n","            json_results[key] = {\"type\": type(value).__name__, \"value\": value}\n","        else:\n","            json_results[key] = {\"type\": type(value).__name__, \"description\": str(value)[:100]}\n","\n","    # Save JSON metadata\n","    with open(json_filename, 'w') as f:\n","        json.dump(json_results, f, indent=2)\n","\n","    save_to_gdrive(json_filename, f\"(Step {step_number} metadata)\")\n","\n","    # Save the actual data as pickle for full restoration\n","    pickle_filename = f\"step_{step_number}_data.pkl\"\n","    with open(pickle_filename, 'wb') as f:\n","        pickle.dump(results_dict, f)\n","\n","    save_to_gdrive(pickle_filename, f\"(Step {step_number} complete data)\")\n","\n","    print(f\"‚úÖ Step {step_number} results saved to Google Drive\")\n","    return json_filename, pickle_filename\n","\n","def load_step_results_from_gdrive(step_number):\n","    \"\"\"\n","    Load step results from Google Drive for session restoration\n","    \"\"\"\n","    import shutil\n","\n","    pickle_filename = f\"step_{step_number}_data.pkl\"\n","    gdrive_path = os.path.join(GDRIVE_OUTPUT_FOLDER, pickle_filename)\n","\n","    if os.path.exists(gdrive_path):\n","        # Copy from Google Drive to local\n","        shutil.copy2(gdrive_path, pickle_filename)\n","\n","        # Load the data\n","        with open(pickle_filename, 'rb') as f:\n","            results = pickle.load(f)\n","\n","        print(f\"‚úÖ Step {step_number} results loaded from Google Drive\")\n","        return results\n","    else:\n","        print(f\"‚ùå Step {step_number} results not found in Google Drive\")\n","        print(f\"   Expected file: {gdrive_path}\")\n","        return None\n","\n","def check_step_completion(step_number):\n","    \"\"\"\n","    Check if a step has been completed and results are available\n","    \"\"\"\n","    pickle_filename = f\"step_{step_number}_data.pkl\"\n","    gdrive_path = os.path.join(GDRIVE_OUTPUT_FOLDER, pickle_filename)\n","\n","    if os.path.exists(gdrive_path):\n","        print(f\"‚úÖ Step {step_number} completed - results available in Google Drive\")\n","        return True\n","    else:\n","        print(f\"‚ùå Step {step_number} not completed - no results found\")\n","        return False\n","\n","def step_0_setup_gdrive():\n","    \"\"\"\n","    Step 0: Setup Google Drive output folder\n","    \"\"\"\n","    print(\"üöÄ STEP 0: SETUP GOOGLE DRIVE OUTPUT FOLDER\")\n","    print(\"=\" * 60)\n","\n","    output_folder = setup_output_folder()\n","    print(f\"üìÅ Output folder configured: {output_folder}\")\n","    print(\"üí° You can change the folder path by modifying GDRIVE_OUTPUT_FOLDER variable\")\n","\n","    # Save step 0 results\n","    step_0_results = {'output_folder': output_folder}\n","    save_step_results_to_gdrive(0, step_0_results)\n","\n","    print(\"‚úÖ Step 0 completed!\")\n","\n","    return output_folder\n","\n","def step_1_create_vs30_grids(bounds=(-9.0, 30.4, -6.8, 31.7)):\n","    \"\"\"\n","    Step 1: Create SRTM-based VS30 grids for both tectonic settings\n","\n","    Parameters:\n","    bounds: tuple (west, south, east, north) - study area bounds\n","\n","    Returns:\n","    tuple: (lons, lats, resolution, vs30_asc_grid, vs30_scc_grid, local_raster_files, gdrive_raster_files)\n","    \"\"\"\n","    print(\"üöÄ STEP 1: CREATE DUAL TECTONIC SETTING VS30 GRIDS\")\n","    print(\"=\" * 60)\n","    print(f\"üìê Study area bounds: {bounds}\")\n","\n","    # Download and process SRTM data\n","    print(\"\\nüì° Downloading SRTM elevation data...\")\n","    lons, lats, resolution = download_srtm_data(bounds)\n","\n","    print(\"\\nüóª Processing elevation data...\")\n","    elevations = get_elevation_grid(lons, lats)\n","\n","    print(\"\\nüìê Calculating slope grid...\")\n","    slope_grid = calculate_slope_grid(elevations, resolution)\n","\n","    print(\"\\nüèîÔ∏è Applying Wald & Allen correlations...\")\n","    vs30_asc_grid, vs30_scc_grid, class_asc_grid, class_scc_grid = apply_wald_allen_correlations_dual(slope_grid, lons, lats)\n","\n","    print(\"\\nüíæ Creating VS30 raster files...\")\n","    local_raster_files, gdrive_raster_files = create_vs30_rasters(vs30_asc_grid, vs30_scc_grid, lons, lats, bounds)\n","\n","    # Save all step 1 results to Google Drive for persistence\n","    step_1_results = {\n","        'lons': lons,\n","        'lats': lats,\n","        'resolution': resolution,\n","        'vs30_asc_grid': vs30_asc_grid,\n","        'vs30_scc_grid': vs30_scc_grid,\n","        'class_asc_grid': class_asc_grid,\n","        'class_scc_grid': class_scc_grid,\n","        'elevations': elevations,\n","        'slope_grid': slope_grid,\n","        'local_raster_files': local_raster_files,\n","        'gdrive_raster_files': gdrive_raster_files,\n","        'bounds': bounds\n","    }\n","\n","    save_step_results_to_gdrive(1, step_1_results)\n","\n","    print(\"‚úÖ Step 1 completed!\")\n","    print(f\"üìÑ Local raster files: {local_raster_files}\")\n","    print(f\"üíæ Google Drive copies: {gdrive_raster_files}\")\n","    print(\"üíæ All step 1 data saved to Google Drive for future sessions\")\n","\n","    return lons, lats, resolution, vs30_asc_grid, vs30_scc_grid, local_raster_files, gdrive_raster_files\n","\n","def step_2_load_and_assign_vs30(local_raster_files=None, resolution=None, use_gdrive_data=True):\n","    \"\"\"\n","    Step 2: Load exposure data and assign VS30 values from both grids\n","\n","    Parameters:\n","    local_raster_files: list of local raster file paths [asc_file, scc_file] (optional if use_gdrive_data=True)\n","    resolution: grid resolution from step 1 (optional if use_gdrive_data=True)\n","    use_gdrive_data: if True, load step 1 results from Google Drive\n","\n","    Returns:\n","    pandas.DataFrame: buildings_df with VS30 assignments for both tectonic settings\n","    \"\"\"\n","    print(\"üöÄ STEP 2: LOAD EXPOSURE DATA AND ASSIGN VS30 VALUES\")\n","    print(\"=\" * 60)\n","\n","    # Load Step 1 results if parameters not provided\n","    if use_gdrive_data or local_raster_files is None or resolution is None:\n","        print(\"üì• Loading Step 1 results from Google Drive...\")\n","        step_1_results = load_step_results_from_gdrive(1)\n","\n","        if step_1_results is None:\n","            print(\"‚ùå Step 1 results not found. Please run step_1_create_vs30_grids() first.\")\n","            return None\n","\n","        # Extract needed parameters\n","        if local_raster_files is None:\n","            local_raster_files = step_1_results['local_raster_files']\n","        if resolution is None:\n","            resolution = step_1_results['resolution']\n","\n","        print(f\"‚úÖ Loaded from Google Drive:\")\n","        print(f\"   Resolution: {resolution}\")\n","        print(f\"   Raster files: {local_raster_files}\")\n","\n","    # Check if local raster files exist, if not copy from Google Drive\n","    import shutil\n","    for i, raster_file in enumerate(local_raster_files):\n","        if not os.path.exists(raster_file):\n","            gdrive_path = os.path.join(GDRIVE_OUTPUT_FOLDER, raster_file)\n","            if os.path.exists(gdrive_path):\n","                shutil.copy2(gdrive_path, raster_file)\n","                print(f\"üìÅ Copied {raster_file} from Google Drive to local directory\")\n","            else:\n","                print(f\"‚ùå Error: {raster_file} not found locally or in Google Drive\")\n","                return None\n","\n","    print(\"\\nüìã Loading building exposure data...\")\n","    buildings_df = load_exposure_data()\n","\n","    print(f\"\\nüéØ Assigning VS30 values using raster files...\")\n","    print(f\"   ASC raster: {local_raster_files[0]}\")\n","    print(f\"   SCC raster: {local_raster_files[1]}\")\n","\n","    buildings_df = assign_vs30_dual_tectonic(buildings_df, local_raster_files[0], local_raster_files[1], resolution)\n","\n","    # Save step 2 results\n","    step_2_results = {\n","        'buildings_df': buildings_df,\n","        'local_raster_files': local_raster_files,\n","        'resolution': resolution\n","    }\n","\n","    save_step_results_to_gdrive(2, step_2_results)\n","\n","    print(\"‚úÖ Step 2 completed!\")\n","    print(f\"üìä Processed {len(buildings_df)} building locations\")\n","    print(\"üíæ Step 2 results saved to Google Drive\")\n","\n","    return buildings_df\n","\n","def step_3_create_outputs(buildings_df=None, bounds=None, use_gdrive_data=True):\n","    \"\"\"\n","    Step 3: Create OpenQuake outputs and validation reports\n","\n","    Parameters:\n","    buildings_df: DataFrame with VS30 assignments (optional if use_gdrive_data=True)\n","    bounds: study area bounds for map creation (optional if use_gdrive_data=True)\n","    use_gdrive_data: if True, load previous step results from Google Drive\n","\n","    Returns:\n","    tuple: (site_model_asc, site_model_scc, summary_report, interactive_map)\n","    \"\"\"\n","    print(\"üöÄ STEP 3: CREATE OPENQUAKE OUTPUTS AND VALIDATION REPORTS\")\n","    print(\"=\" * 60)\n","\n","    # Load previous results if parameters not provided\n","    if use_gdrive_data or buildings_df is None or bounds is None:\n","        print(\"üì• Loading previous step results from Google Drive...\")\n","\n","        # Load Step 2 results for buildings_df\n","        if buildings_df is None:\n","            step_2_results = load_step_results_from_gdrive(2)\n","            if step_2_results is None:\n","                print(\"‚ùå Step 2 results not found. Please run step_2_load_and_assign_vs30() first.\")\n","                return None\n","            buildings_df = step_2_results['buildings_df']\n","\n","        # Load Step 1 results for bounds\n","        if bounds is None:\n","            step_1_results = load_step_results_from_gdrive(1)\n","            if step_1_results is None:\n","                print(\"‚ùå Step 1 results not found. Using default bounds.\")\n","                bounds = (-9.0, 30.4, -6.8, 31.7)\n","            else:\n","                bounds = step_1_results['bounds']\n","\n","        print(f\"‚úÖ Loaded data for {len(buildings_df)} buildings\")\n","\n","    print(\"\\nüè≠ Creating OpenQuake site models...\")\n","    site_model_asc, site_model_scc, asc_file, scc_file = create_openquake_site_models_dual(buildings_df)\n","\n","    print(\"\\nüìä Generating validation summary...\")\n","    summary_report = create_validation_summary_dual(buildings_df)\n","\n","    print(\"\\nüó∫Ô∏è Creating interactive comparison map...\")\n","    interactive_map = create_interactive_map_dual(buildings_df, bounds)\n","\n","    # Save step 3 results\n","    step_3_results = {\n","        'site_model_asc': site_model_asc,\n","        'site_model_scc': site_model_scc,\n","        'summary_report': summary_report,\n","        'asc_file': asc_file,\n","        'scc_file': scc_file,\n","        'bounds': bounds\n","    }\n","\n","    save_step_results_to_gdrive(3, step_3_results)\n","\n","    print(\"‚úÖ Step 3 completed!\")\n","    print(\"üíæ All outputs saved to Google Drive\")\n","\n","    return site_model_asc, site_model_scc, summary_report, interactive_map\n","\n","def step_4_final_summary():\n","    \"\"\"\n","    Step 4: Display final summary of all outputs\n","    \"\"\"\n","    print(\"üöÄ STEP 4: FINAL SUMMARY\")\n","    print(\"=\" * 60)\n","    print(f\"üìÅ All files saved to: {GDRIVE_OUTPUT_FOLDER}\")\n","    print(\"\\nüìã Complete file list:\")\n","    print(\"  üó∫Ô∏è  vs30_grid_morocco_asc.tif - ASC VS30 raster\")\n","    print(\"  üó∫Ô∏è  vs30_grid_morocco_scc.tif - SCC VS30 raster\")\n","    print(\"  üìä site_model_asc.csv - OpenQuake ASC site model\")\n","    print(\"  üìä site_model_scc.csv - OpenQuake SCC site model\")\n","    print(\"  üìà buildings_with_vs30_dual.csv - Complete building data\")\n","    print(\"  üåê vs30_validation_map_dual.html - Interactive map\")\n","    print(\"  üìÑ validation_summary_dual_tectonic.txt - Summary report\")\n","    print(\"  üíæ step_*_data.pkl - Intermediate results for session restoration\")\n","\n","    print(\"\\nüéâ DUAL TECTONIC VS30 ESTIMATION COMPLETED!\")\n","    print(\"üìã Ready for OpenQuake hazard analysis with both ASC and SCC assumptions\")\n","    print(\"üî¨ Use both models for sensitivity analysis and uncertainty quantification\")\n","    print(f\"üíæ Access your files in: {GDRIVE_OUTPUT_FOLDER}\")\n","\n","def check_all_steps():\n","    \"\"\"\n","    Check completion status of all steps\n","    \"\"\"\n","    print(\"üîç CHECKING STEP COMPLETION STATUS\")\n","    print(\"=\" * 50)\n","\n","    step_status = {}\n","    for step_num in range(4):\n","        step_status[step_num] = check_step_completion(step_num)\n","\n","    print(\"\\nüìä SUMMARY:\")\n","    for step_num, completed in step_status.items():\n","        status = \"‚úÖ COMPLETED\" if completed else \"‚ùå NOT COMPLETED\"\n","        print(f\"   Step {step_num}: {status}\")\n","\n","    if all(step_status.values()):\n","        print(\"\\nüéâ All steps completed! You can access all results from Google Drive.\")\n","    else:\n","        incomplete_steps = [i for i, completed in step_status.items() if not completed]\n","        print(f\"\\n‚ö†Ô∏è  Steps {incomplete_steps} need to be completed.\")\n","\n","    return step_status\n","\n","# Global variables to store intermediate results between steps\n","step_results = {}\n","\n","def run_all_steps(bounds=(-9.0, 30.4, -6.8, 31.7)):\n","    \"\"\"\n","    Run all steps in sequence - equivalent to the original main() function\n","\n","    Parameters:\n","    bounds: tuple (west, south, east, north) - study area bounds\n","\n","    Returns:\n","    tuple: (buildings_df, vs30_asc_grid, vs30_scc_grid, site_model_asc, site_model_scc)\n","    \"\"\"\n","    global step_results\n","\n","    print(\"üìä RUNNING ALL STEPS - DUAL TECTONIC SETTING VS30 ESTIMATION\")\n","    print(\"üéØ ASC vs SCC Comparison for Morocco Earthquake Validation Studies\")\n","    print(\"üìê Faithful Implementation of Wald & Allen (2007) + Basin Correlations\")\n","    print(\"üèóÔ∏è  OpenQuake Engine Compatible Output Format\")\n","    print(\"üíæ All outputs automatically saved to Google Drive\")\n","    print(\"=\" * 80)\n","\n","    try:\n","        # Step 0: Setup\n","        output_folder = step_0_setup_gdrive()\n","\n","        # Step 1: Create grids\n","        lons, lats, resolution, vs30_asc_grid, vs30_scc_grid, local_raster_files, gdrive_raster_files = step_1_create_vs30_grids(bounds)\n","\n","        # Step 2: Load and assign\n","        buildings_df = step_2_load_and_assign_vs30(local_raster_files, resolution, use_gdrive_data=False)\n","\n","        # Step 3: Create outputs\n","        site_model_asc, site_model_scc, summary_report, interactive_map = step_3_create_outputs(buildings_df, bounds, use_gdrive_data=False)\n","\n","        # Step 4: Final summary\n","        step_4_final_summary()\n","\n","        return buildings_df, vs30_asc_grid, vs30_scc_grid, site_model_asc, site_model_scc\n","\n","    except Exception as e:\n","        print(f\"\\n‚ùå Error in workflow: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        raise\n","\n","# Convenience functions for easy session restoration\n","def resume_from_step_2():\n","    \"\"\"\n","    Resume workflow from Step 2 (useful if Step 1 was completed in a previous session)\n","    \"\"\"\n","    print(\"üîÑ RESUMING FROM STEP 2\")\n","    print(\"This will load Step 1 results from Google Drive\")\n","    print(\"=\" * 60)\n","\n","    # Check if Step 1 is completed\n","    if not check_step_completion(1):\n","        print(\"‚ùå Step 1 not completed. Please run step_1_create_vs30_grids() first.\")\n","        return None\n","\n","    # Run remaining steps\n","    buildings_df = step_2_load_and_assign_vs30()\n","    if buildings_df is not None:\n","        site_model_asc, site_model_scc, summary_report, interactive_map = step_3_create_outputs()\n","        step_4_final_summary()\n","        return buildings_df\n","    return None\n","\n","def resume_from_step_3():\n","    \"\"\"\n","    Resume workflow from Step 3 (useful if Steps 1-2 were completed in previous sessions)\n","    \"\"\"\n","    print(\"üîÑ RESUMING FROM STEP 3\")\n","    print(\"This will load previous step results from Google Drive\")\n","    print(\"=\" * 60)\n","\n","    # Check if previous steps are completed\n","    if not check_step_completion(1) or not check_step_completion(2):\n","        print(\"‚ùå Previous steps not completed. Please run the missing steps first.\")\n","        return None\n","\n","    # Run remaining steps\n","    site_model_asc, site_model_scc, summary_report, interactive_map = step_3_create_outputs()\n","    step_4_final_summary()\n","    return site_model_asc, site_model_scc\n","\n","# Keep the original main function for backward compatibility\n","def main():\n","    \"\"\"\n","    Execute the complete dual tectonic setting VS30 estimation workflow\n","    This is equivalent to run_all_steps() - kept for backward compatibility\n","    \"\"\"\n","    return run_all_steps()\n","\n","# Execute the workflow\n","if __name__ == \"__main__\":\n","    print(\"üìä DUAL TECTONIC SETTING VS30 ESTIMATION\")\n","    print(\"üéØ ASC vs SCC Comparison for Morocco Earthquake Validation Studies\")\n","    print(\"üìê Faithful Implementation of Wald & Allen (2007) + Basin Correlations\")\n","    print(\"üèóÔ∏è  OpenQuake Engine Compatible Output Format\")\n","    print(\"üíæ All outputs automatically saved to Google Drive\")\n","    print()\n","    print(\"üí° To change the output folder, modify the GDRIVE_OUTPUT_FOLDER variable:\")\n","    print(f\"   Current: {GDRIVE_OUTPUT_FOLDER}\")\n","    print()\n","    print(\"üîß USAGE OPTIONS:\")\n","    print(\"   Option 1 - Run all steps: buildings_data, vs30_asc_data, vs30_scc_data, site_asc, site_scc = run_all_steps()\")\n","    print(\"   Option 2 - Run step by step:\")\n","    print(\"     step_0_setup_gdrive()\")\n","    print(\"     lons, lats, resolution, vs30_asc_grid, vs30_scc_grid, local_files, gdrive_files = step_1_create_vs30_grids()\")\n","    print(\"     buildings_df = step_2_load_and_assign_vs30(local_files, resolution)\")\n","    print(\"     site_asc, site_scc, summary, map_obj = step_3_create_outputs(buildings_df, bounds)\")\n","    print(\"     step_4_final_summary()\")\n","    print()\n","\n","    # Uncomment the line below to run all steps automatically\n","    # buildings_data, vs30_asc_data, vs30_scc_data, site_asc, site_scc = run_all_steps()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZeV7118AsoW","executionInfo":{"status":"ok","timestamp":1752400470756,"user_tz":-60,"elapsed":43,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"e40423ad-5c31-4b09-ee29-448eed3f67e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üìä PROFESSIONAL DUAL TECTONIC SETTING VS30 ESTIMATION\n","üéØ ASC vs SCC Comparison for Morocco Earthquake Validation Studies\n","üìê Faithful Implementation of Wald & Allen (2007) + Basin Correlations\n","üèóÔ∏è  OpenQuake Engine Compatible Output Format\n","üíæ All outputs automatically saved to Google Drive\n","\n","üí° To change the output folder, modify the GDRIVE_OUTPUT_FOLDER variable:\n","   Current: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n","\n","üîß USAGE OPTIONS:\n","   Option 1 - Run all steps: buildings_data, vs30_asc_data, vs30_scc_data, site_asc, site_scc = run_all_steps()\n","   Option 2 - Run step by step:\n","     step_0_setup_gdrive()\n","     lons, lats, resolution, vs30_asc_grid, vs30_scc_grid, local_files, gdrive_files = step_1_create_vs30_grids()\n","     buildings_df = step_2_load_and_assign_vs30(local_files, resolution)\n","     site_asc, site_scc, summary, map_obj = step_3_create_outputs(buildings_df, bounds)\n","     step_4_final_summary()\n","\n"]}]},{"cell_type":"markdown","source":["## 6 - STEP-BY-STEP WORKFLOW EXECUTION"],"metadata":{"id":"L44GE3N_S_gm"}},{"cell_type":"markdown","source":["This section provides flexible execution options for the VS30 estimation workflow, designed to handle long computation times and session interruptions. The workflow is divided into discrete steps that can be run individually or in sequence, with all intermediate results automatically saved to Google Drive for persistence across sessions."],"metadata":{"id":"yIDO-qsAFdxA"}},{"cell_type":"code","source":["step_0_setup_gdrive()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"wPLDBnVEBzdu","executionInfo":{"status":"ok","timestamp":1752400475852,"user_tz":-60,"elapsed":1279,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"8976c2be-13de-414e-9553-9579ec923e76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STEP 0: SETUP GOOGLE DRIVE OUTPUT FOLDER\n","============================================================\n","‚úÖ Using existing output folder: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n","üìÅ Output folder configured: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n","üí° You can change the folder path by modifying GDRIVE_OUTPUT_FOLDER variable\n","üíæ Saved to Google Drive: step_0_results.json (Step 0 metadata)\n","üíæ Saved to Google Drive: step_0_data.pkl (Step 0 complete data)\n","‚úÖ Step 0 results saved to Google Drive\n","‚úÖ Step 0 completed!\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["lons, lats, resolution, vs30_asc_grid, vs30_scc_grid, local_files, gdrive_files = step_1_create_vs30_grids()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Rq8p-rBCFTa","executionInfo":{"status":"ok","timestamp":1752316100858,"user_tz":-60,"elapsed":2450270,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"90eff9ef-5cf3-44e0-b327-12537e501557"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STEP 1: CREATE DUAL TECTONIC SETTING VS30 GRIDS\n","============================================================\n","üìê Study area bounds: (-9.0, 30.4, -6.8, 31.7)\n","\n","üì° Downloading SRTM elevation data...\n","üåç Accessing SRTM elevation data for Morocco study area...\n","üìê Grid dimensions: 266 x 158 cells\n","üîç Resolution: 0.00833 degrees (~900m)\n","\n","üóª Processing elevation data...\n","üîÑ Retrieving SRTM elevation data...\n"]},{"output_type":"stream","name":"stderr","text":["üì° Fetching elevations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4717/4717 [40:47<00:00,  1.93pts/s]\n"]},{"output_type":"stream","name":"stdout","text":["üîß Interpolating to fill data gaps...\n","‚úÖ Elevation grid complete. Range: 175 - 3911 m\n","\n","üìê Calculating slope grid...\n","üìê Calculating topographic slope grid...\n","‚úÖ Slope calculation complete. Range: 0.000000 - 1.331297 m/m\n","\n","üèîÔ∏è Applying Wald & Allen correlations...\n","üèîÔ∏è  Applying Wald & Allen (2007) correlations for dual tectonic settings...\n","üìä ASC: Active Shallow Crust (near Atlas Mountains)\n","üìä SCC: Stable Continental Crust (interior plains)\n","üåã Processing Active Shallow Crust correlations...\n","üèîÔ∏è  Processing Stable Continental Crust correlations...\n","‚úÖ Dual VS30 correlations complete:\n","üìä ASC VS30 range: 150 - 1498 m/s\n","üìä SCC VS30 range: 180 - 1199 m/s\n","üèóÔ∏è  NEHRP class distribution (ASC):\n","   Class B: 27079 cells (64.4%)\n","   Class C1: 5759 cells (13.7%)\n","   Class C2: 2919 cells (6.9%)\n","   Class C3: 3088 cells (7.3%)\n","   Class D1: 72 cells (0.2%)\n","   Class D2: 418 cells (1.0%)\n","   Class D3: 2692 cells (6.4%)\n","   Class E: 1 cells (0.0%)\n","üèóÔ∏è  NEHRP class distribution (SCC):\n","   Class B: 27079 cells (64.4%)\n","   Class C1: 5759 cells (13.7%)\n","   Class C2: 2919 cells (6.9%)\n","   Class C3: 3088 cells (7.3%)\n","   Class D1: 72 cells (0.2%)\n","   Class D2: 418 cells (1.0%)\n","   Class D3: 2692 cells (6.4%)\n","   Class E: 1 cells (0.0%)\n","\n","üíæ Creating VS30 raster files...\n","üíæ Creating VS30 GeoTIFF rasters for both tectonic settings...\n","‚úÖ Created local ASC raster: vs30_grid_morocco_asc.tif\n","‚úÖ Created local SCC raster: vs30_grid_morocco_scc.tif\n","üíæ Saved to Google Drive: vs30_grid_morocco_asc.tif (ASC VS30 raster)\n","üíæ Saved to Google Drive: vs30_grid_morocco_scc.tif (SCC VS30 raster)\n","‚úÖ VS30 rasters created locally and saved to Google Drive\n","üíæ Saved to Google Drive: step_1_results.json (Step 1 metadata)\n","üíæ Saved to Google Drive: step_1_data.pkl (Step 1 complete data)\n","‚úÖ Step 1 results saved to Google Drive\n","‚úÖ Step 1 completed!\n","üìÑ Local raster files: ['vs30_grid_morocco_asc.tif', 'vs30_grid_morocco_scc.tif']\n","üíæ Google Drive copies: ['/content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT/vs30_grid_morocco_asc.tif', '/content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT/vs30_grid_morocco_scc.tif']\n","üíæ All step 1 data saved to Google Drive for future sessions\n"]}]},{"cell_type":"code","source":["buildings_df = resume_from_step_2()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5gBT7mkyOmgh","executionInfo":{"status":"ok","timestamp":1752401096761,"user_tz":-60,"elapsed":9889,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"d3291b8d-df92-46dc-b13b-b82e6c3807ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ RESUMING FROM STEP 2\n","This will load Step 1 results from Google Drive\n","============================================================\n","‚úÖ Step 1 completed - results available in Google Drive\n","üöÄ STEP 2: LOAD EXPOSURE DATA AND ASSIGN VS30 VALUES\n","============================================================\n","üì• Loading Step 1 results from Google Drive...\n","‚úÖ Step 1 results loaded from Google Drive\n","‚úÖ Loaded from Google Drive:\n","   Resolution: 0.008333333333333333\n","   Raster files: ['vs30_grid_morocco_asc.tif', 'vs30_grid_morocco_scc.tif']\n","\n","üìã Loading building exposure data...\n","üìã Loading building exposure data...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-5ce38a77-4f0c-44ab-8227-04bf026cf3bd\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5ce38a77-4f0c-44ab-8227-04bf026cf3bd\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving NWHL6-SH-03-P05_exposure database.csv to NWHL6-SH-03-P05_exposure database.csv\n","‚úÖ Loaded 383 valid building locations\n","üìç Coordinate bounds: Lat [30.468, 31.623], Lon [-8.874, -6.899]\n","\n","üéØ Assigning VS30 values using raster files...\n","   ASC raster: vs30_grid_morocco_asc.tif\n","   SCC raster: vs30_grid_morocco_scc.tif\n","üéØ Assigning VS30 values for both ASC and SCC tectonic settings...\n","üåã Processing Active Shallow Crust assignments...\n","üèîÔ∏è  Processing Stable Continental Crust assignments...\n","‚úÖ Dual tectonic VS30 assignment complete:\n","   ASC: 383/383 buildings assigned, 338 edge-flagged\n","   SCC: 383/383 buildings assigned, 338 edge-flagged\n","   ASC VS30 range: 465 - 1010 m/s\n","   SCC VS30 range: 595 - 1001 m/s\n","üíæ Saved to Google Drive: step_2_results.json (Step 2 metadata)\n","üíæ Saved to Google Drive: step_2_data.pkl (Step 2 complete data)\n","‚úÖ Step 2 results saved to Google Drive\n","‚úÖ Step 2 completed!\n","üìä Processed 383 building locations\n","üíæ Step 2 results saved to Google Drive\n","üöÄ STEP 3: CREATE OPENQUAKE OUTPUTS AND VALIDATION REPORTS\n","============================================================\n","üì• Loading previous step results from Google Drive...\n","‚úÖ Step 2 results loaded from Google Drive\n","‚úÖ Step 1 results loaded from Google Drive\n","‚úÖ Loaded data for 383 buildings\n","\n","üè≠ Creating OpenQuake site models...\n","üè≠ Creating OpenQuake Engine site models for both tectonic settings...\n","üíæ Saved to Google Drive: site_model_asc.csv (ASC OpenQuake site model - 383 sites)\n","üíæ Saved to Google Drive: site_model_scc.csv (SCC OpenQuake site model - 383 sites)\n","‚úÖ OpenQuake site models saved to Google Drive:\n","   ASC model: 'site_model_asc.csv' (383 sites)\n","   SCC model: 'site_model_scc.csv' (383 sites)\n","\n","üìä Sample ASC site model data:\n","   ID        lat       lon  vs30      z1pt0     z2pt5\n","0   1  31.112194 -8.487722   857  20.803873  0.528921\n","1   2  30.979820 -7.103817   767  39.287205  0.600468\n","2   3  31.041500 -7.205611   758  41.978413  0.608896\n","3   4  31.077500 -7.267944   648  91.256157  0.728551\n","4   5  31.210611 -8.182056   890  16.575958  0.506660\n","\n","üìä Sample SCC site model data:\n","   ID        lat       lon  vs30  z1pt0  z2pt5\n","0   1  31.112194 -8.487722   939      0      0\n","1   2  30.979820 -7.103817   903      0      0\n","2   3  31.041500 -7.205611   898      0      0\n","3   4  31.077500 -7.267944   788      0      0\n","4   5  31.210611 -8.182056   953      0      0\n","\n","üìä Generating validation summary...\n","üìä Generating dual tectonic setting validation summary...\n","\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    DUAL TECTONIC SETTING VS30 ESTIMATION VALIDATION SUMMARY\n","    Morocco Earthquake Study - Wald & Allen (2007) Implementation\n","    Active Shallow Crust (ASC) vs Stable Continental Crust (SCC) Comparison\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","\n","    üéØ Methodology:\n","    ‚Ä¢ Grid Resolution: 30 arc-second (~900m) SRTM-based\n","    ‚Ä¢ Tectonic Classifications:\n","      - ASC: Active Shallow Crust (Wald & Allen 2007, Table 1)\n","      - SCC: Stable Continental Crust (Wald & Allen 2007, Table 2)\n","    ‚Ä¢ Basin Depth Correlations:\n","      - ASC z1.0: Chiou & Youngs (2014), z2.5: Campbell & Bozorgnia (2014)\n","      - SCC z1.0: Modified CY14 (65% scaling), z2.5: Modified CB14 (70% scaling)\n","    ‚Ä¢ Coverage: 383 building locations\n","    ‚Ä¢ Valid ASC Assignments: 383 (100.0%)\n","    ‚Ä¢ Valid SCC Assignments: 383 (100.0%)\n","\n","    üìä ACTIVE SHALLOW CRUST (ASC) RESULTS:\n","    ‚Ä¢ Mean VS30: 889 m/s\n","    ‚Ä¢ Median VS30: 905 m/s\n","    ‚Ä¢ Standard Deviation: 78 m/s\n","    ‚Ä¢ Range: 465 - 1010 m/s\n","    ‚Ä¢ Mean z1.0: 20.041 km\n","    ‚Ä¢ Mean z2.5: 0.513 km\n","\n","    üìä STABLE CONTINENTAL CRUST (SCC) RESULTS:\n","    ‚Ä¢ Mean VS30: 951 m/s\n","    ‚Ä¢ Median VS30: 959 m/s\n","    ‚Ä¢ Standard Deviation: 39 m/s\n","    ‚Ä¢ Range: 595 - 1001 m/s\n","    ‚Ä¢ Mean z1.0: 0.000 km\n","    ‚Ä¢ Mean z2.5: 0.000 km\n","\n","    üîÑ TECTONIC SETTING COMPARISON:\n","    ‚Ä¢ VS30 Difference (ASC-SCC): Mean = -62 m/s\n","    ‚Ä¢ z1.0 Difference (ASC-SCC): Mean = 20.041 km\n","    ‚Ä¢ z2.5 Difference (ASC-SCC): Mean = 0.513 km\n","\n","    üèóÔ∏è  NEHRP SITE CLASSIFICATION COMPARISON:\n","    \n","    ASC Classification:\n","      ‚Ä¢ Class B: 378 sites (98.7%)\n","      ‚Ä¢ Class C: 5 sites (1.3%)\n","\n","    SCC Classification:\n","      ‚Ä¢ Class B: 381 sites (99.5%)\n","      ‚Ä¢ Class C: 2 sites (0.5%)\n","\n","    ‚ö†Ô∏è  EDGE EFFECT ANALYSIS:\n","    ‚Ä¢ ASC buildings near grid edges: 338 (88.3%)\n","    ‚Ä¢ SCC buildings near grid edges: 338 (88.3%)\n","    ‚Ä¢ Recommendation: Consider sensitivity analysis for edge-flagged sites\n","    ‚Ä¢ Grid distance range: 383 sites processed\n","\n","    üéØ GMPE COMPATIBILITY:\n","    ‚Ä¢ ASC Compatible GMPEs: Chiou & Youngs (2014), Akkar et al. (2014)\n","    ‚Ä¢ SCC Compatible GMPEs: Atkinson & Boore (2006), Pezeshk et al. (2011)\n","    ‚Ä¢ Basin depth terms included for both settings\n","    ‚Ä¢ Ready for OpenQuake Engine hazard calculations\n","\n","    ‚úÖ VALIDATION STUDY READINESS:\n","    ‚Ä¢ Dual tectonic setting approach enables sensitivity analysis\n","    ‚Ä¢ Consistent methodology applied across both settings\n","    ‚Ä¢ Basin depth estimates included for realistic ground motion prediction\n","    ‚Ä¢ Edge effects identified for quality control\n","    ‚Ä¢ Ready for damage correlation analysis with both ASC and SCC assumptions\n","\n","    üìÅ OUTPUT FILES (Saved to Google Drive):\n","    ‚Ä¢ vs30_grid_morocco_asc.tif: QGIS-compatible ASC VS30 raster\n","    ‚Ä¢ vs30_grid_morocco_scc.tif: QGIS-compatible SCC VS30 raster\n","    ‚Ä¢ site_model_asc.csv: OpenQuake ASC site model (ID, lat, lon, vs30, z1pt0, z2pt5)\n","    ‚Ä¢ site_model_scc.csv: OpenQuake SCC site model (ID, lat, lon, vs30, z1pt0, z2pt5)\n","    ‚Ä¢ buildings_with_vs30_dual.csv: Complete building data with dual assignments\n","    ‚Ä¢ vs30_validation_map_dual.html: Interactive comparison map\n","\n","    ‚ö° RECOMMENDATION FOR HAZARD ANALYSIS:\n","    Morocco's location near the Africa-Eurasia plate boundary suggests ASC conditions\n","    are more appropriate for the Atlas Mountains region, while SCC may apply to\n","    interior areas. Consider using:\n","    ‚Ä¢ ASC for sites within 100km of active faulting/Atlas Mountains\n","    ‚Ä¢ SCC for sites in stable interior regions\n","    ‚Ä¢ Both models for sensitivity analysis and uncertainty quantification\n","\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    \n","üíæ Saved to Google Drive: buildings_with_vs30_dual.csv (Complete building data with dual tectonic assignments)\n","üíæ Saved to Google Drive: validation_summary_dual_tectonic.txt (Validation summary report)\n","‚úÖ Dual tectonic validation summary complete and saved to Google Drive!\n","\n","üó∫Ô∏è Creating interactive comparison map...\n","üó∫Ô∏è  Creating interactive dual tectonic setting comparison map...\n","‚úÖ Interactive dual tectonic map saved as 'vs30_validation_map_dual.html'\n","üíæ Saved to Google Drive: step_3_results.json (Step 3 metadata)\n","üíæ Saved to Google Drive: step_3_data.pkl (Step 3 complete data)\n","‚úÖ Step 3 results saved to Google Drive\n","‚úÖ Step 3 completed!\n","üíæ All outputs saved to Google Drive\n","üöÄ STEP 4: FINAL SUMMARY\n","============================================================\n","üìÅ All files saved to: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n","\n","üìã Complete file list:\n","  üó∫Ô∏è  vs30_grid_morocco_asc.tif - ASC VS30 raster\n","  üó∫Ô∏è  vs30_grid_morocco_scc.tif - SCC VS30 raster\n","  üìä site_model_asc.csv - OpenQuake ASC site model\n","  üìä site_model_scc.csv - OpenQuake SCC site model\n","  üìà buildings_with_vs30_dual.csv - Complete building data\n","  üåê vs30_validation_map_dual.html - Interactive map\n","  üìÑ validation_summary_dual_tectonic.txt - Summary report\n","  üíæ step_*_data.pkl - Intermediate results for session restoration\n","\n","üéâ DUAL TECTONIC VS30 ESTIMATION COMPLETED!\n","üìã Ready for OpenQuake hazard analysis with both ASC and SCC assumptions\n","üî¨ Use both models for sensitivity analysis and uncertainty quantification\n","üíæ Access your files in: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n"]}]},{"cell_type":"code","source":["site_asc, site_scc, summary, map_obj = step_3_create_outputs(buildings_df, (-9.0, 30.4, -6.8, 31.7))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3EeLSSNCQ-x","executionInfo":{"status":"ok","timestamp":1752316207892,"user_tz":-60,"elapsed":826,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"65af539f-0b72-47d4-fe3e-326432c361b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STEP 3: CREATE OPENQUAKE OUTPUTS AND VALIDATION REPORTS\n","============================================================\n","üì• Loading previous step results from Google Drive...\n","‚úÖ Loaded data for 383 buildings\n","\n","üè≠ Creating OpenQuake site models...\n","üè≠ Creating OpenQuake Engine site models for both tectonic settings...\n","üíæ Saved to Google Drive: site_model_asc.csv (ASC OpenQuake site model - 383 sites)\n","üíæ Saved to Google Drive: site_model_scc.csv (SCC OpenQuake site model - 383 sites)\n","‚úÖ OpenQuake site models saved to Google Drive:\n","   ASC model: 'site_model_asc.csv' (383 sites)\n","   SCC model: 'site_model_scc.csv' (383 sites)\n","\n","üìä Sample ASC site model data:\n","   ID        lat       lon  vs30      z1pt0     z2pt5\n","0   1  31.112194 -8.487722   857  20.803873  0.528921\n","1   2  30.979820 -7.103817   767  39.287205  0.600468\n","2   3  31.041500 -7.205611   758  41.978413  0.608896\n","3   4  31.077500 -7.267944   648  91.256157  0.728551\n","4   5  31.210611 -8.182056   890  16.575958  0.506660\n","\n","üìä Sample SCC site model data:\n","   ID        lat       lon  vs30      z1pt0     z2pt5\n","0   1  31.112194 -8.487722   939   7.700032  0.333360\n","1   2  30.979820 -7.103817   903   9.854044  0.348809\n","2   3  31.041500 -7.205611   898  10.204316  0.351081\n","3   4  31.077500 -7.267944   788  22.031385  0.407700\n","4   5  31.210611 -8.182056   953   7.045800  0.328038\n","\n","üìä Generating validation summary...\n","üìä Generating dual tectonic setting validation summary...\n","\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    DUAL TECTONIC SETTING VS30 ESTIMATION VALIDATION SUMMARY\n","    Morocco Earthquake Study - Wald & Allen (2007) Implementation\n","    Active Shallow Crust (ASC) vs Stable Continental Crust (SCC) Comparison\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","\n","    üéØ Methodology:\n","    ‚Ä¢ Grid Resolution: 30 arc-second (~900m) SRTM-based\n","    ‚Ä¢ Tectonic Classifications:\n","      - ASC: Active Shallow Crust (Wald & Allen 2007, Table 1)\n","      - SCC: Stable Continental Crust (Wald & Allen 2007, Table 2)\n","    ‚Ä¢ Basin Depth Correlations:\n","      - ASC z1.0: Chiou & Youngs (2014), z2.5: Campbell & Bozorgnia (2014)\n","      - SCC z1.0: Modified CY14 (65% scaling), z2.5: Modified CB14 (70% scaling)\n","    ‚Ä¢ Coverage: 383 building locations\n","    ‚Ä¢ Valid ASC Assignments: 383 (100.0%)\n","    ‚Ä¢ Valid SCC Assignments: 383 (100.0%)\n","\n","    üìä ACTIVE SHALLOW CRUST (ASC) RESULTS:\n","    ‚Ä¢ Mean VS30: 889 m/s\n","    ‚Ä¢ Median VS30: 905 m/s\n","    ‚Ä¢ Standard Deviation: 78 m/s\n","    ‚Ä¢ Range: 465 - 1010 m/s\n","    ‚Ä¢ Mean z1.0: 20.041 km\n","    ‚Ä¢ Mean z2.5: 0.513 km\n","\n","    üìä STABLE CONTINENTAL CRUST (SCC) RESULTS:\n","    ‚Ä¢ Mean VS30: 951 m/s\n","    ‚Ä¢ Median VS30: 959 m/s\n","    ‚Ä¢ Standard Deviation: 39 m/s\n","    ‚Ä¢ Range: 595 - 1001 m/s\n","    ‚Ä¢ Mean z1.0: 7.560 km\n","    ‚Ä¢ Mean z2.5: 0.330 km\n","\n","    üîÑ TECTONIC SETTING COMPARISON:\n","    ‚Ä¢ VS30 Difference (ASC-SCC): Mean = -62 m/s\n","    ‚Ä¢ z1.0 Difference (ASC-SCC): Mean = 12.481 km\n","    ‚Ä¢ z2.5 Difference (ASC-SCC): Mean = 0.183 km\n","\n","    üèóÔ∏è  NEHRP SITE CLASSIFICATION COMPARISON:\n","    \n","    ASC Classification:\n","      ‚Ä¢ Class B: 378 sites (98.7%)\n","      ‚Ä¢ Class C: 5 sites (1.3%)\n","\n","    SCC Classification:\n","      ‚Ä¢ Class B: 381 sites (99.5%)\n","      ‚Ä¢ Class C: 2 sites (0.5%)\n","\n","    ‚ö†Ô∏è  EDGE EFFECT ANALYSIS:\n","    ‚Ä¢ ASC buildings near grid edges: 338 (88.3%)\n","    ‚Ä¢ SCC buildings near grid edges: 338 (88.3%)\n","    ‚Ä¢ Recommendation: Consider sensitivity analysis for edge-flagged sites\n","    ‚Ä¢ Grid distance range: 383 sites processed\n","\n","    üéØ GMPE COMPATIBILITY:\n","    ‚Ä¢ ASC Compatible GMPEs: Chiou & Youngs (2014), Akkar et al. (2014)\n","    ‚Ä¢ SCC Compatible GMPEs: Atkinson & Boore (2006), Pezeshk et al. (2011)\n","    ‚Ä¢ Basin depth terms included for both settings\n","    ‚Ä¢ Ready for OpenQuake Engine hazard calculations\n","\n","    ‚úÖ VALIDATION STUDY READINESS:\n","    ‚Ä¢ Dual tectonic setting approach enables sensitivity analysis\n","    ‚Ä¢ Consistent methodology applied across both settings\n","    ‚Ä¢ Basin depth estimates included for realistic ground motion prediction\n","    ‚Ä¢ Edge effects identified for quality control\n","    ‚Ä¢ Ready for damage correlation analysis with both ASC and SCC assumptions\n","\n","    üìÅ OUTPUT FILES (Saved to Google Drive):\n","    ‚Ä¢ vs30_grid_morocco_asc.tif: QGIS-compatible ASC VS30 raster\n","    ‚Ä¢ vs30_grid_morocco_scc.tif: QGIS-compatible SCC VS30 raster\n","    ‚Ä¢ site_model_asc.csv: OpenQuake ASC site model (ID, lat, lon, vs30, z1pt0, z2pt5)\n","    ‚Ä¢ site_model_scc.csv: OpenQuake SCC site model (ID, lat, lon, vs30, z1pt0, z2pt5)\n","    ‚Ä¢ buildings_with_vs30_dual.csv: Complete building data with dual assignments\n","    ‚Ä¢ vs30_validation_map_dual.html: Interactive comparison map\n","\n","    ‚ö° RECOMMENDATION FOR HAZARD ANALYSIS:\n","    Morocco's location near the Africa-Eurasia plate boundary suggests ASC conditions\n","    are more appropriate for the Atlas Mountains region, while SCC may apply to\n","    interior areas. Consider using:\n","    ‚Ä¢ ASC for sites within 100km of active faulting/Atlas Mountains\n","    ‚Ä¢ SCC for sites in stable interior regions\n","    ‚Ä¢ Both models for sensitivity analysis and uncertainty quantification\n","\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    \n","üíæ Saved to Google Drive: buildings_with_vs30_dual.csv (Complete building data with dual tectonic assignments)\n","üíæ Saved to Google Drive: validation_summary_dual_tectonic.txt (Validation summary report)\n","‚úÖ Dual tectonic validation summary complete and saved to Google Drive!\n","\n","üó∫Ô∏è Creating interactive comparison map...\n","üó∫Ô∏è  Creating interactive dual tectonic setting comparison map...\n","‚úÖ Interactive dual tectonic map saved as 'vs30_validation_map_dual.html'\n","üíæ Saved to Google Drive: step_3_results.json (Step 3 metadata)\n","üíæ Saved to Google Drive: step_3_data.pkl (Step 3 complete data)\n","‚úÖ Step 3 results saved to Google Drive\n","‚úÖ Step 3 completed!\n","üíæ All outputs saved to Google Drive\n"]}]},{"cell_type":"code","source":["step_4_final_summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4utRuDyOCSal","executionInfo":{"status":"ok","timestamp":1752316260107,"user_tz":-60,"elapsed":47,"user":{"displayName":"Maxime Chollet","userId":"07944469866231838532"}},"outputId":"a4007f86-1478-41d6-8cf6-048624b152e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STEP 4: FINAL SUMMARY\n","============================================================\n","üìÅ All files saved to: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n","\n","üìã Complete file list:\n","  üó∫Ô∏è  vs30_grid_morocco_asc.tif - ASC VS30 raster\n","  üó∫Ô∏è  vs30_grid_morocco_scc.tif - SCC VS30 raster\n","  üìä site_model_asc.csv - OpenQuake ASC site model\n","  üìä site_model_scc.csv - OpenQuake SCC site model\n","  üìà buildings_with_vs30_dual.csv - Complete building data\n","  üåê vs30_validation_map_dual.html - Interactive map\n","  üìÑ validation_summary_dual_tectonic.txt - Summary report\n","  üíæ step_*_data.pkl - Intermediate results for session restoration\n","\n","üéâ DUAL TECTONIC VS30 ESTIMATION COMPLETED!\n","üìã Ready for OpenQuake hazard analysis with both ASC and SCC assumptions\n","üî¨ Use both models for sensitivity analysis and uncertainty quantification\n","üíæ Access your files in: /content/drive/MyDrive/IRDR0012_Research Project/01 OUTPUT\n"]}]}]}